<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Computer Vision Lecture 9</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="28da013d-48bb-4a89-94aa-af277a52de10" class="page sans"><header><h1 class="page-title">Computer Vision Lecture 9</h1><table class="properties"><tbody><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesDate"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></span>Date</th><td><time>@December 10, 2022</time></td></tr><tr class="property-row property-row-number"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesNumber"><path d="M2.4834 10.9902H4.33594L3.75488 13.8887C3.74121 13.9639 3.72754 14.0664 3.72754 14.1416C3.72754 14.5381 3.99414 14.7637 4.39746 14.7637C4.81445 14.7637 5.09473 14.5449 5.18359 14.1143L5.80566 10.9902H8.79297L8.21191 13.8887C8.19824 13.9639 8.18457 14.0664 8.18457 14.1416C8.18457 14.5381 8.45117 14.7637 8.85449 14.7637C9.27148 14.7637 9.55176 14.5449 9.63379 14.1143L10.2627 10.9902H12.4502C12.9287 10.9902 13.2432 10.6758 13.2432 10.2109C13.2432 9.8418 12.9902 9.56836 12.6006 9.56836H10.5498L11.2129 6.28711H13.3662C13.8379 6.28711 14.1523 5.96582 14.1523 5.50098C14.1523 5.13184 13.9062 4.8584 13.5098 4.8584H11.5L12.0195 2.27441C12.0264 2.21973 12.0469 2.11035 12.0469 2.01465C12.0469 1.625 11.7666 1.39258 11.3633 1.39258C10.9053 1.39258 10.6797 1.63867 10.5977 2.05566L10.0303 4.8584H7.04297L7.5625 2.27441C7.57617 2.21973 7.58984 2.11035 7.58984 2.01465C7.58984 1.625 7.30957 1.39258 6.91309 1.39258C6.44824 1.39258 6.21582 1.63867 6.13379 2.05566L5.57324 4.8584H3.54297C3.06445 4.8584 2.75 5.18652 2.75 5.65137C2.75 6.03418 3.00293 6.28711 3.39258 6.28711H5.28613L4.62305 9.56836H2.63379C2.15527 9.56836 1.84082 9.89648 1.84082 10.3613C1.84082 10.7373 2.09375 10.9902 2.4834 10.9902ZM6.09277 9.56836L6.75586 6.28711H9.74316L9.08008 9.56836H6.09277Z"></path></svg></span>Number</th><td>9</td></tr><tr class="property-row property-row-text"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M1.56738 3.25879H14.4258C14.7676 3.25879 15.0479 2.97852 15.0479 2.63672C15.0479 2.29492 14.7744 2.02148 14.4258 2.02148H1.56738C1.21875 2.02148 0.952148 2.29492 0.952148 2.63672C0.952148 2.97852 1.22559 3.25879 1.56738 3.25879ZM1.56738 6.84082H14.4258C14.7676 6.84082 15.0479 6.56055 15.0479 6.21875C15.0479 5.87695 14.7744 5.60352 14.4258 5.60352H1.56738C1.21875 5.60352 0.952148 5.87695 0.952148 6.21875C0.952148 6.56055 1.22559 6.84082 1.56738 6.84082ZM1.56738 10.4229H14.4258C14.7676 10.4229 15.0479 10.1426 15.0479 9.80078C15.0479 9.45898 14.7744 9.18555 14.4258 9.18555H1.56738C1.21875 9.18555 0.952148 9.45898 0.952148 9.80078C0.952148 10.1426 1.22559 10.4229 1.56738 10.4229ZM1.56738 14.0049H8.75879C9.10059 14.0049 9.38086 13.7246 9.38086 13.3828C9.38086 13.041 9.10742 12.7676 8.75879 12.7676H1.56738C1.21875 12.7676 0.952148 13.041 0.952148 13.3828C0.952148 13.7246 1.22559 14.0049 1.56738 14.0049Z"></path></svg></span>Topics</th><td>Object Detection</td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM7.62402 10.6348C7.79492 10.915 8.20508 10.9287 8.37598 10.6348L10.666 6.73145C10.8574 6.41016 10.7002 6.04102 10.3652 6.04102H5.62793C5.29297 6.04102 5.14941 6.43066 5.32031 6.73145L7.62402 10.6348Z"></path></svg></span>Type</th><td><span class="selected-value select-value-color-orange">Lecture</span></td></tr></tbody></table></header><div class="page-body"><nav id="542a15fd-449e-413c-b0c7-aa31908d0855" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#4a2e98d8-4a98-4781-961d-717e213bd3c5">Object Detection</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9cdaf28a-6463-4437-a811-b2dfbd98e286">Datasets</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#dfc963b8-049e-4753-a8c3-4e1fbfc58e77">Face detection</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#8739cfc6-0b9b-43f2-8f5a-76dd72114858">Pedestrians</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#d64d726f-3e07-4e68-ae06-0a774346df9e">PASCAL VOC</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#8b47cf24-9cc8-4a24-aef3-d9faf2f47d8d">COCO</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1aeaa336-c024-4414-97dd-efd316f52e26">Evaluation Metric</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#7884dd67-158d-4ae4-8c29-aec475985bef">IoU (Intersection over Union)</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#6176d5f8-ead9-4332-ba2a-5510fca41420">Precision</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#8b42b272-4de0-437c-9b39-fd915856c004">Recall </a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#42abaaba-7b6d-4cbb-a809-9b15e2b987ad">Tradeoff between Precision and Recall</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#e81bde9f-c101-4763-88df-3b0f95482350">Average Precision</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#7e9761fc-16bd-4f10-a17a-3fff18740840"><em><em><em><em><em><em><em><em>Average </em></em></em></em></em></em></em></em>average precision</a></div></nav><h1 id="4a2e98d8-4a98-4781-961d-717e213bd3c5" class="">Object Detection</h1><p id="74dd988a-decf-4030-a7f3-fc2acb59e6c2" class="">The object detection task is defined as finding bounding boxes around objects of interest. For example, in the following image, the objective was to find two classes in the image: person and horse, and the results returned were those of 4 bounding boxes around the persons and the horses in the image.</p><figure id="e74b66b6-0fd3-4207-b397-68973803d177" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled.png"><img style="width:384px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled.png"/></a></figure><p id="a54fd64f-071c-457d-9bb0-df56bd51d367" class=""><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em><em>Given a certain class, the requirement is to return a bounding box around all instances of that class.</em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></em></p><h2 id="9cdaf28a-6463-4437-a811-b2dfbd98e286" class="">Datasets</h2><h3 id="dfc963b8-049e-4753-a8c3-4e1fbfc58e77" class="">Face detection</h3><p id="9e8e009b-4e7f-41c4-9d94-eec023629854" class="">One of the most popular problems in object detection is detecting faces. This dataset, which was used in the 1990s, had one category, face, and they were all frontal, with no distortions, little deformation (fairly rigid and expressionless, side views, or occlusions (no eyewear or anything hiding the face).</p><p id="c2be8c8b-cf1f-4fd2-9394-3b6403658eb2" class="">At the time the problem was not that easy but efficient solutions existed.</p><h3 id="8739cfc6-0b9b-43f2-8f5a-76dd72114858" class="">Pedestrians</h3><p id="5550f08d-e044-415d-9378-6a8c6d4e8982" class="">In the 2000s another problem was explored, that of pedestrians. The focus was also on one category. The problem was harder than that of face detection as pedestrians are not as rigid as faces; there were slight pose variations, small distortions, and partial occlusions.</p><h3 id="d64d726f-3e07-4e68-ae06-0a774346df9e" class="">PASCAL VOC</h3><p id="ddbdc448-5e30-4b3f-84c3-c9b8727da7f5" class="">From 2007 to 2012 there was a popular challenge associated with this dataset, <a href="http://host.robots.ox.ac.uk/pascal/VOC/#:~:text=The%20PASCAL%20VOC%20project%3A,and%20comparison%20of%20different%20methods">Pascal Visual Object Classes</a>. It was well annotated, with 20 categories and 10,000 images. Three problems were solved using it: classification, detection and segmentation.</p><p id="7505dd97-63e6-4314-95fb-0724a06d2590" class="">It had large pose variations, heavy occlusions, and generic scenes. Since the dataset got more complicated, there was a need for a unified performance metric and a benchmark to properly compare results.</p><h3 id="8b47cf24-9cc8-4a24-aef3-d9faf2f47d8d" class="">COCO</h3><p id="ca8cd50f-3f3a-4905-8e1b-4604cce2945c" class="">Microsoftâ€™s <a href="https://cocodataset.org/#home">Common Objects in Context</a> dataset was conceived after the PASCAL challenge was closed. It had many more classes, 80 diverse categories, with 100,000 images. Papers on it solved the same 3 problems, classification, detection and segmentation. It featured heavy occlusions, many objects per image (unlike the PASCAL dataset, where there was a limited number of object classes and instances per image), and large scale variations.</p><h2 id="1aeaa336-c024-4414-97dd-efd316f52e26" class="">Evaluation Metric</h2><p id="01467cb3-c855-4084-9da6-276fd1ee7db1" class="">So your model outputs a detection. How to evaluate this detection against a ground truth? Some metrics are defined for the object detection problem.</p><figure id="2a114259-b23f-4f2a-ae78-1fb3d17acc7e" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%201.png"><img style="width:480px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%201.png"/></a><figcaption>ground truth</figcaption></figure><figure id="b0a3d78a-0b34-4359-ba3d-df90d47d1385" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%202.png"><img style="width:480px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%202.png"/></a><figcaption>detections</figcaption></figure><p id="c441a331-45dc-4093-b9ec-46df9a4e7c1b" class="">Some predictions may be inaccurate (detecting an object where there are none), missing an object, or returning an ill-fitted bounding box (bounding box is too big, or surrounds only part of the object). A <em><em><em><em><em><em><em>quantitative measure </em></em></em></em></em></em></em>is required for object detection model. Some metrics are defined for the object detection problem:</p><h3 id="7884dd67-158d-4ae4-8c29-aec475985bef" class="">IoU (Intersection over Union)</h3><p id="b78c489b-b07c-4364-9a5a-80581b8e6b95" class="">Compare each detection with each of the ground truths and computer the intersection over union (the area in common divided by the sum of areas of the two bounding boxes).</p><figure id="e9fc443e-fd98-4a79-b7bf-6c9e0431dc1d" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%203.png"><img style="width:480px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%203.png"/></a></figure><p id="ac2dd750-1894-4ef1-ad2c-3ae54ad0fb01" class="">Bounding boxes that have the largest overlap between them (a high IoU value) are matched to each other. Intuitively, if a returned bounding box has a lot in common with one of the ground truth boxes, then both of them must be detecting the same thing. Notice that if there are multiple detections for the same object, multiple boxes will match to the same ground truth box, so there is a need for non-maximum suppression here (or perhaps some method to merge the detections). The IoU metric can help with this.</p><p id="8ce46a57-d178-4bb3-9fd2-257bc01bf8a6" class="">An empirical threshold to true/false detection is an IoU of 0.5. If IoU &gt; 0.5, it is deemed to be a correct detection (this does not prevent from there being multiple detections for the same object, each with an IoU surpassing this threshold).</p><h3 id="6176d5f8-ead9-4332-ba2a-5510fca41420" class="">Precision</h3><div id="b5b07898-a9d3-4d4e-9939-cb44a80dfd4d" class="column-list"><div id="662e5a68-555d-4a3c-b07f-a578efe7e7ef" style="width:50%" class="column"><p id="d0dff75f-9d3c-4255-8c91-a70be2ea8448" class="">Precision is defined as the number of correct detections / total detections. The correct detections can be counted using the IoU. It represents how good the model is at finding accurate boxes.</p><p id="d6ecea87-3330-4e2d-8f60-100e999e949a" class="">Here if we consider that there is only one correct detection (assuming the IoU of the box on the right was below  0.5), precision would be equal 1/3.</p></div><div id="50e613e8-1822-4877-83c1-89998dc3b34a" style="width:50%" class="column"><figure id="b176a7ea-3671-4224-b0b1-db9dd1842c8a" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%204.png"><img style="width:198px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%204.png"/></a></figure></div></div><h3 id="8b42b272-4de0-437c-9b39-fd915856c004" class="">Recall </h3><div id="8ecb7d9b-572f-449b-80d0-0bc3917099b1" class="column-list"><div id="d00fe59c-1676-4e20-aa32-e1e26ac30028" style="width:50%" class="column"><p id="e00b056f-c616-470b-8b63-e887038260b1" class="">Recall is defined as the number of ground truth with matched detections / total ground truth. It represents how good the model is at detecting <em><em><em><em>all </em></em></em></em>the objects in the image.</p><p id="1ad0aa56-a980-4951-b6ba-d39dba3d68b1" class="">Here recall would be only 0.5 as only one of the two bounding boxes were returned. Note that precision here is equal to 1, as the one detection that was returned was correct.</p></div><div id="a1ddc7e1-9243-4eae-9c82-be2894fedaa1" style="width:50%" class="column"><figure id="36974e4f-e5c2-49c5-9302-85c73413477a" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%205.png"><img style="width:221px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%205.png"/></a></figure></div></div><h3 id="42abaaba-7b6d-4cbb-a809-9b15e2b987ad" class="">Tradeoff between Precision and Recall</h3><div id="855f795a-874f-4a83-9a72-2e6a5ceef445" class="column-list"><div id="12c42f2f-5576-4299-a01a-b6d7093ce9c1" style="width:50%" class="column"><p id="9f07d22c-b4cf-43e0-a616-79f92396658e" class="">Precision and recall both require that the predictions be matched to ground truths using IoU, but their objectives are at odds with each others.</p><p id="92a62c1a-ecee-4c37-9fb1-892476d6c18b" class="">A model would return scores for detections. We should then weed out weak predictions by setting a threshold. If the threshold is too low, too many detections will be retained, lowering the precision score. Note that this however <em><em><em><em><em><em><em><em><em><em>increases </em></em></em></em></em></em></em></em></em></em>recall, as the possibility to miss anything decreases.</p></div><div id="27928066-6eb7-4523-beda-983c3c6fb4f1" style="width:50%" class="column"><figure id="c86bc4a4-3ef3-4499-bcd4-da4a5de58870" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%206.png"><img style="width:192px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%206.png"/></a><figcaption>Precision = 1; Recall = 1</figcaption></figure></div></div><p id="55f9ccc0-c1d9-44d1-9e24-022dd461999d" class="">On the flip side, if we set the threshold to be too high, we can be sure that the remaining detections are what the model is fairly sure of, so the precision will be high. However, the recall will be low as it is more likely to miss objects this way.</p><p id="284dcf12-b6b5-4bd1-91ca-99c2bc9deca9" class="">So how should we set the threshold? Which metric is more important? The right tradeoff depends largely on the application. If the model is for example, detecting cancer cells in the tissue, an erroneous positive has much less severe consequences than missing a cancerous cell, so recall here is more important. If we are trying to detect edible mushrooms in a forest, one incorrect detection could be fatal, so we can choose to miss a few edible ones over detecting poisonous ones as edible, as precision here needs to be high.</p><h3 id="e81bde9f-c101-4763-88df-3b0f95482350" class="">Average Precision</h3><figure id="601f85e8-c3c5-4e4b-8099-b5869ed28ca7" class="image"><a href="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%207.png"><img style="width:336px" src="Computer%20Vision%20Lecture%209%2028da013d48bb4a8994aaaf277a52de10/Untitled%207.png"/></a></figure><p id="3f12b72c-2476-4983-befa-640f30b15e3e" class="">This graph visualizes the tradeoff between precision and recall. We get the values for it when we change the threshold (as threshold decreases, precision decreases, and recall increases). So when the threshold is 100%, precision is 1 and recall is at its worst. When the threshold is ~0%, recall is 1 and precision is at its worst. The area under this curve is called the <em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong><em><strong>Average Precision.</strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></strong></em></p><h3 id="7e9761fc-16bd-4f10-a17a-3fff18740840" class=""><em><em><em><em><em><em><em><em>Average </em></em></em></em></em></em></em></em>average precision</h3><p id="776dd812-1474-4c39-a6c3-a7ba4ff3c1a8" class="">
</p></div></article></body></html>